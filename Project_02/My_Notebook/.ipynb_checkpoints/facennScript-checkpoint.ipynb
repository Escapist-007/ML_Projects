{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Comparing single layer MLP with deep MLP (using TensorFlow)\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import logistic\n",
    "from math import sqrt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this\n",
    "def initializeWeights(n_in,n_out):\n",
    "    \"\"\"\n",
    "    # initializeWeights return the random weights for Neural Network given the\n",
    "    # number of node in the input layer and output layer\n",
    "\n",
    "    # Input:\n",
    "    # n_in: number of nodes of the input layer\n",
    "    # n_out: number of nodes of the output layer\n",
    "                            \n",
    "    # Output: \n",
    "    # W: matrix of random initial weights with size (n_out x (n_in + 1))\"\"\"\n",
    "    epsilon = sqrt(6) / sqrt(n_in + n_out + 1);\n",
    "    W = (np.random.rand(n_out, n_in + 1)*2* epsilon) - epsilon;\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1.0 / (1.0 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnObjFunction(params, *args):\n",
    "    \n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    \n",
    "    obj_val = 0\n",
    "    n = training_data.shape[0]\n",
    "    ''' \n",
    "                                Step 01: Feedforward Propagation \n",
    "    '''\n",
    "    \n",
    "    '''Input Layer --> Hidden Layer\n",
    "    '''\n",
    "    # Adding bias node to every training data. Here, the bias value is 1 for every training data\n",
    "    # A training data is a feature vector X. \n",
    "    # We have 717 features for every training data\n",
    "\n",
    "    biases1 = np.full((n,1), 1)\n",
    "    training_data_bias = np.concatenate((biases1, training_data),axis=1)\n",
    "    \n",
    "    # aj is the linear combination of input data and weight (w1) at jth hidden node. \n",
    "    # Here, 1 <= j <= no_of_hidden_units\n",
    "    aj = np.dot( training_data_bias, np.transpose(w1))\n",
    "    \n",
    "    # zj is the output from the hidden unit j after applying sigmoid as an activation function\n",
    "    zj = sigmoid(aj)\n",
    "    \n",
    "    '''Hidden Layer --> Output Layer\n",
    "    '''\n",
    "    \n",
    "    # Adding bias node to every zj. \n",
    "    \n",
    "    m = zj.shape[0]\n",
    "    \n",
    "    biases2 = np.full((m,1), 1)\n",
    "    zj_bias = np.concatenate((biases2, zj), axis=1)\n",
    "    \n",
    "    # bl is the linear combination of hidden units output and weight(w2) at lth output node. \n",
    "    # Here, l = 10 as we are classifying 10 digits\n",
    "    bl = np.dot(zj_bias, np.transpose(w2))\n",
    "    ol = sigmoid(bl)\n",
    "    \n",
    "    ''' \n",
    "                            Step 2:  Error Calculation by error function\n",
    "    '''\n",
    "    # yl --> Ground truth for every training dataset\n",
    "    yl = np.full((n, n_class), 0)\n",
    "\n",
    "    for i in range(n):\n",
    "        trueLabel = training_label[i]\n",
    "        yl[i][trueLabel] = 1\n",
    "    \n",
    "    yl_prime = (1.0-yl)\n",
    "    ol_prime = (1.0-ol)\n",
    "    \n",
    "    lol = np.log(ol)\n",
    "    lol_prime = np.log(ol_prime)\n",
    "    \n",
    "    # Our Error function is \"negative log-likelihood\"\n",
    "    # We need elementwise multiplication between the matrices\n",
    "    \n",
    "    error = np.sum( np.multiply(yl,lol) + np.multiply(yl_prime,lol_prime) )/((-1)*n)\n",
    "\n",
    "#     error = -np.sum( np.sum(yl*lol + yl_prime*lol_prime, 1))/ n\n",
    "    \n",
    "    ''' \n",
    "                         Step 03: Gradient Calculation for Backpropagation of error\n",
    "    '''\n",
    "    \n",
    "    delta = ol- yl\n",
    "    gradient_w2 = np.dot(delta.T, zj_bias)\n",
    "   \n",
    "    temp = np.dot(delta,w2) * ( zj_bias * (1.0-zj_bias))\n",
    "    \n",
    "    gradient_w1 = np.dot( np.transpose(temp), training_data_bias)\n",
    "    gradient_w1 = gradient_w1[1:, :]\n",
    "    \n",
    "    ''' \n",
    "                                Step 04: Regularization \n",
    "    '''\n",
    "    regularization =  lambdaval * (np.sum(w1**2) + np.sum(w2**2)) / (2*n)\n",
    "    obj_val = error + regularization\n",
    "    \n",
    "    gradient_w1_reg = (gradient_w1 + lambdaval * w1)/n\n",
    "    gradient_w2_reg = (gradient_w2 + lambdaval * w2)/n\n",
    "\n",
    "    obj_grad = np.concatenate((gradient_w1_reg.flatten(), gradient_w2_reg.flatten()), 0)\n",
    "\n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(w1, w2, training_data):\n",
    "\n",
    "    n = training_data.shape[0]\n",
    "\n",
    "    biases1 = np.full((n,1),1)\n",
    "    training_data = np.concatenate((biases1, training_data), axis=1)\n",
    "\n",
    "    aj = np.dot(training_data, w1.T)\n",
    "    zj = sigmoid(aj)\n",
    "    \n",
    "    m = zj.shape[0]\n",
    "    \n",
    "    biases2 = np.full((m,1), 1)\n",
    "    zj = np.concatenate((biases2, zj), axis=1)\n",
    "\n",
    "    bl = np.dot(zj, w2.T)\n",
    "    ol = sigmoid(bl)\n",
    "\n",
    "    labels = np.argmax(ol, axis=1)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:85.77251184834124%\n",
      "\n",
      " Validation set Accuracy:84.765478424015%\n",
      "\n",
      " Test set Accuracy:86.26040878122635%\n",
      "Training Time: 48.06817364692688\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Do not change this\n",
    "def preprocess():\n",
    "    pickle_obj = pickle.load(file=open('face_all.pickle', 'rb'))\n",
    "    features = pickle_obj['Features']\n",
    "    labels = pickle_obj['Labels']\n",
    "    train_x = features[0:21100] / 255\n",
    "    valid_x = features[21100:23765] / 255\n",
    "    test_x = features[23765:] / 255\n",
    "\n",
    "    labels = labels[0]\n",
    "    train_y = labels[0:21100]\n",
    "    valid_y = labels[21100:23765]\n",
    "    test_y = labels[23765:]\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "\"\"\"**************Neural Network Script Starts here********************************\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "#  Train Neural Network\n",
    "\n",
    "trainingStart = time.time()\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 256\n",
    "# set the number of nodes in output unit\n",
    "n_class = 2\n",
    "\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden);\n",
    "initial_w2 = initializeWeights(n_hidden, n_class);\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()),0)\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 10;\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "#Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "opts = {'maxiter' :50}    # Preferred value.\n",
    "\n",
    "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args,method='CG', options=opts)\n",
    "params = nn_params.get('x')\n",
    "#Reshape nnParams from 1D vector into w1 and w2 matrices\n",
    "w1 = params[0:n_hidden * (n_input + 1)].reshape( (n_hidden, (n_input + 1)))\n",
    "w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "#Test the computed parameters\n",
    "predicted_label = nnPredict(w1,w2,train_data)\n",
    "#find the accuracy on Training Dataset\n",
    "print('\\n Training set Accuracy:' + str(100*np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "predicted_label = nnPredict(w1,w2,validation_data)\n",
    "#find the accuracy on Validation Dataset\n",
    "print('\\n Validation set Accuracy:' + str(100*np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "predicted_label = nnPredict(w1,w2,test_data)\n",
    "#find the accuracy on Validation Dataset\n",
    "print('\\n Test set Accuracy:' +  str(100*np.mean((predicted_label == test_label).astype(float))) + '%')\n",
    "trainingEnd = time.time()\n",
    "\n",
    "print('Training Time:',(trainingEnd-trainingStart))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
